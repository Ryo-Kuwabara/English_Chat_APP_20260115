import streamlit as st
import os
import time
from time import sleep
from pathlib import Path
from streamlit.components.v1 import html
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chains import ConversationChain
from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain.schema import SystemMessage
from openai import OpenAI
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from pydub import AudioSegment
import functions as ft
import constants as ct


# å„ç¨®è¨­å®š
load_dotenv()
st.set_page_config(
    page_title=ct.APP_NAME
)

# ã‚¿ã‚¤ãƒˆãƒ«è¡¨ç¤º
st.markdown(f"## {ct.APP_NAME}")

# åˆæœŸå‡¦ç†
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.mode = ct.MODE_1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ¼ãƒ‰
    st.session_state.speed = 1.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé€Ÿåº¦
    st.session_state.current_step = "waiting"  # waiting, recording, processing
    st.session_state.recorded_audio = None
    
    # éŒ²éŸ³ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆç”¨ã®åˆæœŸåŒ–
    st.session_state.global_microphone_permission = False
    
    st.session_state.openai_obj = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
    st.session_state.llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)
    st.session_state.memory = ConversationSummaryBufferMemory(
        llm=st.session_state.llm,
        max_token_limit=1000,
        return_messages=True
    )

    # ãƒ¢ãƒ¼ãƒ‰ã€Œæ—¥å¸¸è‹±ä¼šè©±ã€ç”¨ã®Chainä½œæˆ
    st.session_state.chain_basic_conversation = ft.create_chain(ct.SYSTEM_TEMPLATE_BASIC_CONVERSATION)

# UIè¨­å®š
st.session_state.mode = st.selectbox(
    "ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠ", 
    options=[ct.MODE_1, ct.MODE_2], 
    index=0,
    help="ç·´ç¿’ã—ãŸã„ãƒ¢ãƒ¼ãƒ‰ã‚’é¸ã‚“ã§ãã ã•ã„"
)

st.session_state.speed = st.selectbox(
    "å†ç”Ÿé€Ÿåº¦", 
    options=ct.PLAY_SPEED_OPTION, 
    index=3,
    format_func=lambda x: f"{x}x"
)

with st.chat_message("assistant", avatar="images/ai_icon.jpg"):
    st.markdown("ã“ã¡ã‚‰ã¯ç”ŸæˆAIã«ã‚ˆã‚‹éŸ³å£°è‹±ä¼šè©±ã®ç·´ç¿’ã‚¢ãƒ—ãƒªã§ã™ã€‚ä½•åº¦ã‚‚ç¹°ã‚Šè¿”ã—ç·´ç¿’ã—ã€è‹±èªåŠ›ã‚’ã‚¢ãƒƒãƒ—ã•ã›ã¾ã—ã‚‡ã†ã€‚")
    st.markdown("**ã€æ“ä½œèª¬æ˜ã€‘**")
    st.info("""
    ğŸ“± **ä½¿ã„æ–¹**:
    1. ãƒ¢ãƒ¼ãƒ‰ã¨å†ç”Ÿé€Ÿåº¦ã‚’é¸æŠ
    2. **ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³1å›ç›®ã®ã‚¯ãƒªãƒƒã‚¯**: éŒ²éŸ³é–‹å§‹
    3. **è‹±èªã§è©±ã™** (å¥½ããªã ã‘é•·æ™‚é–“OK)
    4. **ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³2å›ç›®ã®ã‚¯ãƒªãƒƒã‚¯**: éŒ²éŸ³åœæ­¢
    5. AIãŒå¿œç­”ã‚’è‡ªå‹•éŸ³å£°å†ç”Ÿã—ã¾ã™
    
    ğŸ’¡ **ãƒã‚¤ãƒ³ãƒˆ**: 
    - éŒ²éŸ³æ™‚é–“ã¯è‡ªåˆ†ã§ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«å¯èƒ½
    - åˆå›ã®ã¿ãƒã‚¤ã‚¯ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ãŒå¿…è¦
    """)

# ãƒã‚¤ã‚¯ãƒ†ã‚¹ãƒˆ
st.markdown("### ğŸ¤ ãƒã‚¤ã‚¯ãƒ†ã‚¹ãƒˆ")
test_audio = ft.record_audio_simple("test")
if test_audio is not None and len(test_audio) > 100:
    st.success("âœ… ãƒã‚¤ã‚¯ãƒ†ã‚¹ãƒˆæˆåŠŸï¼éŒ²éŸ³æ©Ÿèƒ½ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
elif test_audio is not None:
    st.warning("âš ï¸ éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸãŒçŸ­ã™ãã¾ã™ã€‚éŒ²éŸ³é–‹å§‹â†’è©±ã™â†’éŒ²éŸ³åœæ­¢ã®æµã‚Œã§ãƒ†ã‚¹ãƒˆã—ã¦ãã ã•ã„ã€‚")
else:
    if st.session_state.get("global_microphone_permission", False):
        st.info("â¬†ï¸ ä¸Šã®ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã§éŒ²éŸ³ãƒ†ã‚¹ãƒˆã‚’ã—ã¦ãã ã•ã„ã€‚")
    else:
        st.info("â¬†ï¸ ä¸Šã®ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãƒã‚¤ã‚¯ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ã‚’è¡Œã£ã¦ãã ã•ã„ï¼ˆåˆå›ã®ã¿ï¼‰ã€‚")

st.divider()

# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã®ä¸€è¦§è¡¨ç¤ºï¼ˆæœ€æ–°ã®ä¼šè©±ã®ã¿è¡¨ç¤ºï¼‰
if st.session_state.messages:
    # æœ€æ–°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ã‚’è¡¨ç¤º
    latest_messages = st.session_state.messages[-2:] if len(st.session_state.messages) >= 2 else st.session_state.messages
    for idx, message in enumerate(latest_messages):
        actual_idx = len(st.session_state.messages) - len(latest_messages) + idx
        if message["role"] == "assistant":
            with st.chat_message(message["role"], avatar="images/ai_icon.jpg"):
                st.markdown(message["content"])
                # æ—¥å¸¸è‹±ä¼šè©±ãƒ¢ãƒ¼ãƒ‰ã§ã‹ã¤AIãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒé–¢é€£ä»˜ã‘ã•ã‚Œã¦ã„ã‚‹å ´åˆ
                if (st.session_state.mode == ct.MODE_1 and 
                    "audio_path" in message and 
                    message["audio_path"] and 
                    os.path.exists(message["audio_path"])):
                    
                    col_msg_replay1, col_msg_replay2 = st.columns([1, 4])
                    with col_msg_replay1:
                        # å„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”¨ã®ä¸€æ„ãªã‚­ãƒ¼ã‚’ç”Ÿæˆ
                        replay_key = f"replay_latest_{actual_idx}"
                        if st.button("ğŸ”Š å†èª­ã¿ä¸Šã’", key=replay_key, use_container_width=True):
                            success = ft.play_audio_web_compatible(message["audio_path"], st.session_state.speed)
                            if success:
                                st.toast("éŸ³å£°ã‚’å†ç”Ÿã—ã¾ã—ãŸ", icon="ğŸ”Š")
                            else:
                                st.toast("éŸ³å£°å†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸ", icon="âŒ")
        elif message["role"] == "user":
            with st.chat_message(message["role"], avatar="images/user_icon.jpg"):
                st.markdown(message["content"])

# ãƒ¡ã‚¤ãƒ³æ©Ÿèƒ½
st.markdown("### ğŸ—£ï¸ éŸ³å£°è‹±ä¼šè©±ç·´ç¿’")

# ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—è¡¨ç¤º
if st.session_state.current_step == "waiting":
    if st.session_state.get("global_microphone_permission", False):
        st.info("ğŸ¤ **éŒ²éŸ³é–‹å§‹**: ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ â†’ è©±ã™ â†’ **éŒ²éŸ³åœæ­¢**: ã‚‚ã†ä¸€åº¦ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯")
    else:
        st.warning("ğŸ“± ãƒã‚¤ã‚¯ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ãŒå¿…è¦ã§ã™ï¼ˆåˆå›ã®ã¿ï¼‰ã€‚ä¸‹ã®ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãƒ–ãƒ©ã‚¦ã‚¶ã§ã€Œè¨±å¯ã€ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        
        # Safariå°‚ç”¨ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚’è¿½åŠ 
        with st.expander("ğŸ Safariåˆ©ç”¨ã®æ–¹ã¸ - æ¯å›è¨±å¯ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹å ´åˆ"):
            st.markdown("""
            **Safari ã§æ¯å›è¨±å¯ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹å ´åˆã®è§£æ±ºæ–¹æ³•:**
            
            1. **ã‚µã‚¤ãƒˆè¨­å®šã‚’ç¢ºèª**:
               - ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒãƒ¼å·¦ã®ã€ŒğŸ”’ã€ã¾ãŸã¯ã€ŒAAã€ã‚’ã‚¯ãƒªãƒƒã‚¯
               - ã€ŒWebã‚µã‚¤ãƒˆã®è¨­å®šã€ã‚’é¸æŠ
               - ã€Œãƒã‚¤ã‚¯ã€ã‚’ã€Œè¨±å¯ã€ã«è¨­å®š
            
            2. **ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿**ã—ã¦ã‹ã‚‰ã”åˆ©ç”¨ãã ã•ã„
            
            3. ãã‚Œã§ã‚‚è§£æ±ºã—ãªã„å ´åˆã¯ **Chrome** ã¾ãŸã¯ **Edge** ã®ä½¿ç”¨ã‚’ãŠå‹§ã‚ã—ã¾ã™
            """)
elif st.session_state.current_step == "recording":
    st.warning("ğŸ”´ **éŒ²éŸ³ä¸­...** è©±ã—çµ‚ã‚ã£ãŸã‚‰ **ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’ã‚‚ã†ä¸€åº¦ã‚¯ãƒªãƒƒã‚¯** ã—ã¦åœæ­¢ã—ã¦ãã ã•ã„")
elif st.session_state.current_step == "processing":
    st.info("âš™ï¸ éŸ³å£°ã‚’å‡¦ç†ä¸­... ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„")

# éŒ²éŸ³æ©Ÿèƒ½ï¼ˆå¸¸ã«è¡¨ç¤ºã€ãŸã ã—å‡¦ç†ä¸­ã¯ç„¡åŠ¹åŒ–è¡¨ç¤ºï¼‰
recorded_audio = ft.record_audio_simple("main")

# éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
if recorded_audio is not None and len(recorded_audio) > 50:  # æœ€å°ãƒã‚¤ãƒˆæ•°ã‚’ç·©å’Œï¼ˆ100â†’50ï¼‰
    # æ–°ã—ã„éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã‹ã¤ã€ç¾åœ¨å‡¦ç†ä¸­ã§ãªã„å ´åˆã®ã¿å‡¦ç†é–‹å§‹
    if (st.session_state.recorded_audio != recorded_audio and 
        st.session_state.current_step == "waiting"):
        st.session_state.recorded_audio = recorded_audio
        st.session_state.current_step = "processing"
        st.rerun()

# éŸ³å£°å‡¦ç†ï¼ˆprocessingçŠ¶æ…‹ã®å ´åˆã®ã¿ï¼‰
if st.session_state.current_step == "processing" and st.session_state.recorded_audio:
    # å‡¦ç†é–‹å§‹å‰ã«éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ï¼ˆé‡è¤‡å‡¦ç†ã‚’é˜²ãï¼‰
    current_audio = st.session_state.recorded_audio
    st.session_state.recorded_audio = None
    
    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    audio_input_file_path = f"{ct.AUDIO_INPUT_DIR}/audio_input_{int(time.time())}.wav"
    
    if ft.save_audio_to_file(current_audio, audio_input_file_path):
        # éŸ³å£°èªè­˜
        with st.spinner('éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ä¸­...'):
            transcript = ft.transcribe_audio(audio_input_file_path)
            audio_input_text = transcript.text

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’è¡¨ç¤º
        with st.chat_message("user", avatar=ct.USER_ICON_PATH):
            st.markdown(audio_input_text)

        # ãƒ¢ãƒ¼ãƒ‰åˆ¥å‡¦ç†
        if st.session_state.mode == ct.MODE_1:  # æ—¥å¸¸è‹±ä¼šè©±
            # AIå¿œç­”ç”Ÿæˆ
            with st.spinner("AIå¿œç­”ã‚’ç”Ÿæˆä¸­..."):
                llm_response = st.session_state.chain_basic_conversation.predict(input=audio_input_text)
                
                # éŸ³å£°åˆæˆ
                llm_response_audio = st.session_state.openai_obj.audio.speech.create(
                    model="tts-1",
                    voice="alloy",
                    input=llm_response
                )

                # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ãƒ»å†ç”Ÿ
                audio_output_file_path = f"{ct.AUDIO_OUTPUT_DIR}/audio_output_{int(time.time())}.wav"
                ft.save_to_wav(llm_response_audio.content, audio_output_file_path)
                
                # AIå¿œç­”ã‚’è¡¨ç¤º
                with st.chat_message("assistant", avatar=ct.AI_ICON_PATH):
                    st.markdown(llm_response)
                    st.info("ğŸ”Š éŸ³å£°ã‚’è‡ªå‹•å†ç”Ÿä¸­...")
                
                # Webæ¨™æº–ã®ãƒ–ãƒ©ã‚¦ã‚¶éŸ³å£°å†ç”Ÿï¼ˆlocalhost/ã‚¯ãƒ©ã‚¦ãƒ‰ä¸¡å¯¾å¿œï¼‰
                print(f"[MAIN] WebéŸ³å£°å†ç”Ÿé–‹å§‹: {audio_output_file_path}")
                
                # ãƒ–ãƒ©ã‚¦ã‚¶ã§ã®éŸ³å£°å†ç”Ÿ
                success = ft.play_audio_web_compatible(audio_output_file_path, st.session_state.speed)
                
                if success:
                    st.success("ğŸ”Š éŸ³å£°å†ç”Ÿå®Œäº†ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶å†ç”Ÿï¼‰")
                else:
                    st.error("âŒ éŸ³å£°å†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸ")
                
                # å†èª­ã¿ä¸Šã’ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ï¼ˆä¸€æ„ãªãƒ•ã‚¡ã‚¤ãƒ«åã§ï¼‰
                timestamp = int(time.time())
                saved_audio_path = f"{ct.AUDIO_OUTPUT_DIR}/audio_saved_{timestamp}.wav"
                audio_for_save = AudioSegment.from_wav(audio_output_file_path)
                audio_for_save.export(saved_audio_path, format="wav")
                
                # ã“ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å°‚ç”¨ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ä¿å­˜
                current_message_audio_path = saved_audio_path
                
                # å°‘ã—é…å»¶ã—ã¦ã‹ã‚‰å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                import threading
                def delayed_cleanup():
                    time.sleep(3)  # 3ç§’å¾Œã«å‰Šé™¤
                    if os.path.exists(audio_output_file_path):
                        try:
                            os.remove(audio_output_file_path)
                        except:
                            pass
                
                threading.Thread(target=delayed_cleanup, daemon=True).start()

            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã«è¿½åŠ ï¼ˆæ­£ã—ã„éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã§ï¼‰
            st.session_state.messages.append({"role": "user", "content": audio_input_text})
            st.session_state.messages.append({
                "role": "assistant", 
                "content": llm_response,
                "audio_path": current_message_audio_path
            })

        elif st.session_state.mode == ct.MODE_2:  # ã‚·ãƒ£ãƒ‰ãƒ¼ã‚¤ãƒ³ã‚°
            # ã‚·ãƒ£ãƒ‰ãƒ¼ã‚¤ãƒ³ã‚°ç”¨ã®å‡¦ç†ï¼ˆç°¡ç´ åŒ–ï¼‰
            st.info("ã‚·ãƒ£ãƒ‰ãƒ¼ã‚¤ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã¯ä»Šå¾Œå®Ÿè£…äºˆå®šã§ã™")

        # å‡¦ç†å®Œäº†å¾Œã®çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ
        st.session_state.current_step = "waiting"
        # éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã‚‚ã‚¯ãƒªã‚¢
        st.session_state.recorded_audio = None
        # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        st.success("âœ… éŸ³å£°å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚æ¬¡ã®éŒ²éŸ³ã‚’ã©ã†ãï¼")
        
        # UIæ›´æ–°ã®ãŸã‚ã«å†å®Ÿè¡Œï¼ˆéŒ²éŸ³ãƒœã‚¿ãƒ³ã‚’å†è¡¨ç¤ºï¼‰
        st.rerun()
        
    else:
        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã«å¤±æ•—ã—ãŸå ´åˆ
        st.error("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦éŒ²éŸ³ã—ã¦ãã ã•ã„ã€‚")
        st.session_state.current_step = "waiting"
        st.session_state.recorded_audio = None
        # UIæ›´æ–°ã®ãŸã‚ã«å†å®Ÿè¡Œ
        st.rerun()

st.divider()

# ä¼šè©±å±¥æ­´è¡¨ç¤ºï¼ˆå…¨å±¥æ­´ï¼‰
if len(st.session_state.messages) > 2:
    st.markdown("### ğŸ“ ä¼šè©±å±¥æ­´")
    for idx, message in enumerate(st.session_state.messages[:-2]):  # æœ€æ–°2ä»¶ä»¥å¤–ã‚’è¡¨ç¤º
        if message["role"] == "assistant":
            with st.chat_message(message["role"], avatar="images/ai_icon.jpg"):
                st.markdown(message["content"])
                # å†èª­ã¿ä¸Šã’ãƒœã‚¿ãƒ³
                if "audio_path" in message and message["audio_path"] and os.path.exists(message["audio_path"]):
                    if st.button("ğŸ”Š å†èª­ã¿ä¸Šã’", key=f"history_replay_{idx}", use_container_width=True):
                        success = ft.play_audio_web_compatible(message["audio_path"], st.session_state.speed)
                        if success:
                            st.toast("éŸ³å£°ã‚’å†ç”Ÿã—ã¾ã—ãŸ", icon="ğŸ”Š")
                        else:
                            st.toast("éŸ³å£°å†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸ", icon="âŒ")
                else:
                    st.caption("âš ï¸ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        elif message["role"] == "user":
            with st.chat_message(message["role"], avatar="images/user_icon.jpg"):
                st.markdown(message["content"])